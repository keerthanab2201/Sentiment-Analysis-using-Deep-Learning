{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLc4uc4hCRgiRT7KwTQo4/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keerthanab2201/Sentiment-Analysis-using-Deep-Learning/blob/main/BERT_cnn_lstm_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Collection"
      ],
      "metadata": {
        "id": "-R4GteA2nue0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfbfPXXVpMsa",
        "outputId": "ac3524ab-a94f-4cb2-def6-a0edd31b1231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load csv file and preview\n",
        "import pandas as pd\n",
        "df= pd.read_csv(\"/content/drive/MyDrive/datasets/Amazon-Product-Reviews-Sentiment-Analysis-in-Python-Dataset.csv\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6ur-XpFn6vr",
        "outputId": "4947b0a6-caf9-4a42-8143-6a5014dd119f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Review  Sentiment\n",
            "0  Fast shipping but this product is very cheaply...          1\n",
            "1  This case takes so long to ship and it's not e...          1\n",
            "2  Good for not droids. Not good for iPhones. You...          1\n",
            "3  The cable was not compatible between my macboo...          1\n",
            "4  The case is nice but did not have a glow light...          1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save as a JSON file(records format)\n",
        "df.to_json(\"amazon_reviews_data.json\", orient=\"records\", lines=True)\n",
        "print(\"✅ Conversion complete: Saved as reviews_data.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3hM71jOn-2G",
        "outputId": "f56f2e61-73bb-4ece-ded5-4b35e6f37c77"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Conversion complete: Saved as reviews_data.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Pre-Processing\n",
        "\n",
        "* lowercase\n",
        "* stopword removal\n",
        "* punctuation removal\n",
        "* one word review removal\n",
        "* contraction removal\n",
        "* tokenization\n",
        "* part of speech tagging"
      ],
      "metadata": {
        "id": "qPVR83iyoC8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing dependencies\n",
        "!pip install contractions textblob gensim beautifulsoup4\n",
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13TM3g9qoGH4",
        "outputId": "b8ad41c1-08ce-4aef-a874-cbad0b6cd5b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JyIqYeHohZ5",
        "outputId": "cdbfddd2-c315-4292-ac3b-50a4980976be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import Modules\n",
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import gensim\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from contractions import fix as expand_contractions\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "aKS2O-uUomeS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load JSON dataset and inspect columns\n",
        "df = pd.read_json(\"amazon_reviews_data.json\", lines=True)\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCIKOirfonT5",
        "outputId": "54de292a-80f1-4122-b44a-10f632b41622"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Review', 'Sentiment'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop missing values and filter required columns\n",
        "text_col = \"Review\"\n",
        "label_col = \"Sentiment\"\n",
        "df = df[[text_col, label_col]].dropna() #these are the two columns\n",
        "df.columns = [\"text\", \"rating\"]  # Normalize column names"
      ],
      "metadata": {
        "id": "ZgZMhV2tophx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocessing function\n",
        "def preprocess_pipeline(text):\n",
        "    text = str(text)\n",
        "    text = expand_contractions(text.lower())            # Lowercase + contractions\n",
        "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)          # Elongated word normalization\n",
        "    tokens = word_tokenize(text)                        # Tokenization\n",
        "    # Optional: Stopwords (skip for deep models)\n",
        "    # tokens = [t for t in tokens if t not in stop_words]\n",
        "    tokens = [re.sub(r\"[^\\w\\s!?]\", \"\", t) for t in tokens]  # 5. Remove selective punctuation\n",
        "    tokens = [t for t in tokens if t.strip() != \"\"]\n",
        "    if len(tokens) <= 1:\n",
        "        return None\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    clean_text = \" \".join(tokens)\n",
        "    polarity_score = TextBlob(clean_text).sentiment.polarity\n",
        "    return {\n",
        "        \"clean_text\": clean_text,\n",
        "        \"tokens\": tokens,\n",
        "        \"pos_tags\": pos_tags,\n",
        "        \"score\": polarity_score\n",
        "    }\n"
      ],
      "metadata": {
        "id": "qP8sbho2osjt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntdpb224pRs-",
        "outputId": "60d31aca-aaf4-4031-a84e-d9d02e59792d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing function\n",
        "processed = df[\"text\"].apply(preprocess_pipeline)\n",
        "df = df[processed.notnull()].copy()\n",
        "df[\"processed\"] = processed[processed.notnull()].values"
      ],
      "metadata": {
        "id": "HqHyCafmo6Pd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep text and label\n",
        "df = df[df['rating'] != 3]\n",
        "df['label'] = df['rating'].apply(lambda x: 1 if x >= 4 else 0)\n",
        "\n",
        "texts = df[\"processed\"].apply(lambda x: x[\"clean_text\"]).tolist()\n",
        "labels = df[\"label\"].tolist()"
      ],
      "metadata": {
        "id": "_hOSB350nvUV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract cleaned data for tokenization\n",
        "texts = df[\"processed\"].apply(lambda x: x[\"clean_text\"]).tolist()\n",
        "labels = df[\"rating\"].tolist()\n",
        "scores = df[\"processed\"].apply(lambda x: x[\"score\"]).tolist()\n",
        "# Result: Three lists containing the text data, labels, and scores respectively"
      ],
      "metadata": {
        "id": "zGKp9etso84a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDFcT57Mp_rr",
        "outputId": "4495630a-baad-4e80-c89c-64fcc3b41295"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geOYBdFJnziE",
        "outputId": "1cd089e1-6b5a-47ff-9222-83e352bf24ca"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and encode using BERT\n",
        "encoded_inputs = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n"
      ],
      "metadata": {
        "id": "I0vCznndpAhZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate BERT embeddings (without gradient tracking)\n",
        "import torch\n",
        "with torch.no_grad():\n",
        "    bert_outputs = bert_model(**encoded_inputs)"
      ],
      "metadata": {
        "id": "GEd_CWcRpDdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use [CLS] token embedding as sentence embedding\n",
        "X = bert_outputs.last_hidden_state[:, 0, :].numpy()  # shape: (num_samples, 768)\n",
        "y = np.array(labels)"
      ],
      "metadata": {
        "id": "NaIpCvHYpHL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split- 70% training / 10% validation / 20% testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Split 20% test set\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    padded_sequences, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# Step 2: Split remaining 80% into 70% train and 10% validation\n",
        "# 70% of full data = 87.5% of remaining 80%\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.125, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"Train size     : {len(X_train)}\")\n",
        "print(f\"Validation size: {len(X_val)}\")\n",
        "print(f\"Test size      : {len(X_test)}\")\n"
      ],
      "metadata": {
        "id": "wmZ4DHZhCv-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CNN-LSTM Model"
      ],
      "metadata": {
        "id": "QbJAy64iELmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, LSTM, BatchNormalization, Bidirectional, Reshape\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "SeaLQSPOEmr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Attention Layer\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(shape=(input_shape[-1], 1),\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "    def call(self, inputs):\n",
        "        score = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * inputs\n",
        "        return tf.reduce_sum(context_vector, axis=1)"
      ],
      "metadata": {
        "id": "8OnUeF04EqMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model hyperparameters\n",
        "embedding_dim = 768        # BERT output dim\n",
        "sequence_length = 1        # 1 CLS embedding per sample\n",
        "num_filters = 128\n",
        "kernel_size = 3\n",
        "pool_size = 2\n",
        "strides = 1\n",
        "lstm_units = 64\n",
        "dropout_rate = 0.4"
      ],
      "metadata": {
        "id": "XLdHact2Etgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape BERT CLS embedding to (batch_size, timesteps=1, features=768)\n",
        "X_train_reshaped = X_train.reshape(X_train.shape[0], 1, 768)\n",
        "X_val_reshaped = X_val.reshape(X_val.shape[0], 1, 768)"
      ],
      "metadata": {
        "id": "1A5EWfa9ognh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model\n",
        "cnn_lstm_model = Sequential()\n",
        "cnn_lstm_model.add(Conv1D(filters=num_filters, kernel_size=1, strides=1, activation='relu', input_shape=(1, 768)))\n",
        "cnn_lstm_model.add(BatchNormalization())\n",
        "cnn_lstm_model.add(MaxPooling1D(pool_size=1))\n",
        "\n",
        "cnn_lstm_model.add(Bidirectional(LSTM(units=lstm_units, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)))\n",
        "cnn_lstm_model.add(Attention())\n",
        "cnn_lstm_model.add(Dropout(dropout_rate))\n",
        "\n",
        "cnn_lstm_model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "cnn_lstm_model.add(Dropout(0.4))\n",
        "cnn_lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "cnn_lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "mfJdXi5OollQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, verbose=1, min_lr=1e-6)\n",
        "checkpoint = ModelCheckpoint(\"best_cnn_lstm_bert_model.h5\", monitor='val_loss', save_best_only=True, verbose=1)\n"
      ],
      "metadata": {
        "id": "idZz-oN9orJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = cnn_lstm_model.fit(\n",
        "    X_train_reshaped, np.array(y_train),\n",
        "    batch_size=32,\n",
        "    epochs=20,\n",
        "    validation_data=(X_val_reshaped, np.array(y_val)),\n",
        "    callbacks=[early_stop, lr_scheduler, checkpoint],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "F_bsaJieFB_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Evaluation"
      ],
      "metadata": {
        "id": "-rCZpCFmFOIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Predict\n",
        "y_pred_prob = cnn_lstm_model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Confusion Matrix (CNN-LSTM):\")\n",
        "print(cm)\n",
        "print(f\"\\nAccuracy     : {accuracy:.4f}\")\n",
        "print(f\"Precision    : {precision:.4f}\")\n",
        "print(f\"Recall       : {recall:.4f}\")\n",
        "print(f\"F1 Score     : {f1:.4f}\")\n",
        "print(f\"Specificity  : {specificity:.4f}\")"
      ],
      "metadata": {
        "id": "nAOieXw9FK7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot accuracy and loss vs epoch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy vs Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot Loss\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss vs Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jivD2_ZbFdit"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}