{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjkVHoJzLImXh4PMdKka5V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keerthanab2201/Sentiment-Analysis-using-Deep-Learning/blob/main/new_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Collection"
      ],
      "metadata": {
        "id": "-R4GteA2nue0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfbfPXXVpMsa",
        "outputId": "a6a7638c-d529-442e-a915-b9ae8480f4e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load csv file and preview\n",
        "import pandas as pd\n",
        "df= pd.read_csv(\"/content/drive/MyDrive/datasets/Amazon-Product-Reviews-Sentiment-Analysis-in-Python-Dataset.csv\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6ur-XpFn6vr",
        "outputId": "e5b2c135-7413-48fe-bf95-42172d0c908a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Review  Sentiment\n",
            "0  Fast shipping but this product is very cheaply...          1\n",
            "1  This case takes so long to ship and it's not e...          1\n",
            "2  Good for not droids. Not good for iPhones. You...          1\n",
            "3  The cable was not compatible between my macboo...          1\n",
            "4  The case is nice but did not have a glow light...          1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save as a JSON file(records format)\n",
        "df.to_json(\"amazon_reviews_data.json\", orient=\"records\", lines=True)\n",
        "print(\"âœ… Conversion complete: Saved as reviews_data.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3hM71jOn-2G",
        "outputId": "7713a5b6-714b-40f9-aab6-e2913291379c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Conversion complete: Saved as reviews_data.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Pre-Processing\n",
        "\n",
        "* lowercase\n",
        "* stopword removal\n",
        "* punctuation removal\n",
        "* one word review removal\n",
        "* contraction removal\n",
        "* tokenization\n",
        "* part of speech tagging"
      ],
      "metadata": {
        "id": "qPVR83iyoC8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing dependencies\n",
        "!pip install contractions textblob gensim beautifulsoup4\n",
        "!python -m textblob.download_corpora\n",
        "!pip uninstall -y tensorflow keras transformers\n",
        "!pip install --force-reinstall \\\n",
        "    tensorflow==2.10.1 \\\n",
        "    transformers==4.36.2 \\\n",
        "    datasets==2.16.1 \\\n",
        "    evaluate==0.4.1 \\\n",
        "    tensorflow-addons==0.20.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13TM3g9qoGH4",
        "outputId": "e61b0a7c-a5c2-4247-b092-4ccf285048ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n",
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n",
            "Found existing installation: keras 3.8.0\n",
            "Uninstalling keras-3.8.0:\n",
            "  Successfully uninstalled keras-3.8.0\n",
            "Found existing installation: transformers 4.54.0\n",
            "Uninstalling transformers-4.54.0:\n",
            "  Successfully uninstalled transformers-4.54.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.10.1 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0, 2.20.0rc0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.10.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JyIqYeHohZ5",
        "outputId": "6e629a72-0ee1-41f3-d818-463b91d5d06c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import Modules\n",
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import gensim\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from contractions import fix as expand_contractions\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "from transformers import DataCollatorWithPadding\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "id": "aKS2O-uUomeS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "outputId": "718953a7-d196-400c-e23b-71e5a71f8dca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.src.engine'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2204979209.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Local project imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Additional activation functions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardshrink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhardshrink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisht\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlisht\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/activations/gelu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorLike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# New versions of Keras require importing from `keras.src` when\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# importing internal symbols.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2.5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.src.engine'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GPU:\", tf.config.list_physical_devices('GPU'))\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
      ],
      "metadata": {
        "id": "5pwz8bu0T3UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load JSON dataset and inspect columns\n",
        "df = pd.read_json(\"amazon_reviews_data.json\", lines=True)\n",
        "print(df.columns)"
      ],
      "metadata": {
        "id": "OCIKOirfonT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop missing values and filter required columns\n",
        "text_col = \"Review\"\n",
        "label_col = \"Sentiment\"\n",
        "df = df[[text_col, label_col]].dropna() #these are the two columns\n",
        "df.columns = [\"text\", \"rating\"]  # Normalize column names"
      ],
      "metadata": {
        "id": "ZgZMhV2tophx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocessing function\n",
        "def preprocess_pipeline(text):\n",
        "    text = str(text)\n",
        "    text = expand_contractions(text.lower())            # Lowercase + contractions\n",
        "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)          # Elongated word normalization\n",
        "    tokens = word_tokenize(text)                        # Tokenization\n",
        "    # Optional: Stopwords (skip for deep models)\n",
        "    # tokens = [t for t in tokens if t not in stop_words]\n",
        "    tokens = [re.sub(r\"[^\\w\\s!?]\", \"\", t) for t in tokens]  # 5. Remove selective punctuation\n",
        "    tokens = [t for t in tokens if t.strip() != \"\"]\n",
        "    if len(tokens) <= 1:\n",
        "        return None\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    clean_text = \" \".join(tokens)\n",
        "    polarity_score = TextBlob(clean_text).sentiment.polarity\n",
        "    return {\n",
        "        \"clean_text\": clean_text,\n",
        "        \"tokens\": tokens,\n",
        "        \"pos_tags\": pos_tags,\n",
        "        \"score\": polarity_score\n",
        "    }\n"
      ],
      "metadata": {
        "id": "qP8sbho2osjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "LApm0TbMjVtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing function\n",
        "processed = df[\"text\"].apply(preprocess_pipeline)\n",
        "df = df[processed.notnull()].copy()\n",
        "df[\"processed\"] = processed[processed.notnull()].values"
      ],
      "metadata": {
        "id": "HqHyCafmo6Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract cleaned data for tokenization\n",
        "texts = df[\"processed\"].apply(lambda x: x[\"clean_text\"]).tolist()\n",
        "labels = df[\"rating\"].tolist()\n",
        "scores = df[\"processed\"].apply(lambda x: x[\"score\"]).tolist()\n",
        "# Result: Three lists containing the text data, labels, and scores respectively"
      ],
      "metadata": {
        "id": "zGKp9etso84a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Embeddings\n",
        "# Keras Tokenizer- converts raw text into numerical sequences (each word= unique integer index) that can be later processed by Keras layers like Embedding\n",
        "MAX_VOCAB = 10000\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=\"\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "# sequence padding- adding placeholder values (often zeros) to shorter sequences in a dataset to make them all the same length\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "padded_sequences = pad_sequences(sequences, maxlen=200, padding='post')"
      ],
      "metadata": {
        "id": "I0vCznndpAhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/glove.6B.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"glove\")"
      ],
      "metadata": {
        "id": "GEd_CWcRpDdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size= 15000\n",
        "# Load GloVe and Create Embedding Matrix\n",
        "#GloVe(Global Vectors for Word Representation)- converts words into numerical vectors(embeddings) that capture semantic relationships between words- unsupervised learning algorithm\n",
        "\n",
        "embedding_index = {}\n",
        "with open(\"glove/glove.6B.100d.txt\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "embedding_dim = 100  # matching model spec\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < vocab_size:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "NaIpCvHYpHL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove neutral reviews and relabel\n",
        "df = df[df['rating'] != 3]\n",
        "df['label'] = df['rating'].apply(lambda x: 1 if x >= 4 else 0) # creates binary labels: 1 for positive (rating â‰¥4), 0 for negative"
      ],
      "metadata": {
        "id": "NHxLE1ITBy1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cleaned texts again\n",
        "texts = df[\"processed\"].apply(lambda x: x[\"clean_text\"]).tolist()\n",
        "labels = df[\"label\"].tolist()"
      ],
      "metadata": {
        "id": "7Ey15NRuB5ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare tokenizer and sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "FHWAYnT7CJip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 15000\n",
        "input_length = 400\n",
        "# creates vocabulary of 15,000 most frequent words, converts text to integer sequences and pads sequences to length 400 (truncating longer texts or padding shorter ones)"
      ],
      "metadata": {
        "id": "ZYwivu_QCNkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='')\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "padded_sequences = pad_sequences(sequences, maxlen=input_length, padding='post')\n",
        "\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "fkxmxiElCQp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup embedding matrix\n",
        "# initializes embedding matrix with zeros and fill it with pre-trained GloVe vectors where available\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= vocab_size:\n",
        "        continue\n",
        "    vec = embedding_index.get(word)\n",
        "    if vec is not None:\n",
        "        embedding_matrix[i] = vec"
      ],
      "metadata": {
        "id": "sA05B9ZGCt7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split- 70% training / 10% validation / 20% testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Split 20% test set\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    padded_sequences, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# Step 2: Split remaining 80% into 70% train and 10% validation\n",
        "# 70% of full data = 87.5% of remaining 80%\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.125, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"Train size     : {len(X_train)}\")\n",
        "print(f\"Validation size: {len(X_val)}\")\n",
        "print(f\"Test size      : {len(X_test)}\")\n"
      ],
      "metadata": {
        "id": "wmZ4DHZhCv-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TCN Model"
      ],
      "metadata": {
        "id": "qxZ7kWzKi-jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Embedding, Conv1D, BatchNormalization, Activation,\n",
        "    Add, Dropout, Dense, Multiply, Permute, Lambda, Flatten, RepeatVector\n",
        ")\n",
        "from tcn import TCN\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "u_5k5eioi5X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        add_special_tokens=True\n",
        "    )\n",
        "\n",
        "# Convert to datasets (as in your original)\n",
        "train_dataset = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
        "val_dataset = Dataset.from_dict({\"text\": X_val, \"label\": y_val})\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "xVYVUh_oUznc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== MODEL OPTIMIZATIONS =====\n",
        "# Hyperparameters tuned for 95%+ accuracy\n",
        "MAX_LENGTH = 256  # Increased from your original 128\n",
        "BATCH_SIZE = 16    # Reduced for better gradient estimates\n",
        "LEARNING_RATE = 1.5e-5\n",
        "EPOCHS = 4\n",
        "WARMUP_STEPS = 500"
      ],
      "metadata": {
        "id": "2z6Yeq8UUr_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== KEY MODIFICATIONS FOR HIGHER ACCURACY =====\n",
        "# 1. Custom Model Architecture\n",
        "def build_enhanced_model():\n",
        "    # Load base model (preserving your approach)\n",
        "    base_model = TFDistilBertForSequenceClassification.from_pretrained(\n",
        "        'distilbert-base-uncased',\n",
        "        num_labels=2\n",
        "    )\n",
        "\n",
        "    # Build custom classifier head\n",
        "    input_ids = tf.keras.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name=\"input_ids\")\n",
        "    attention_mask = tf.keras.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "    # Enhanced architecture\n",
        "    distilbert_output = base_model.distilbert(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
        "    pooled_output = tf.keras.layers.GlobalAveragePooling1D()(distilbert_output)\n",
        "    dropout = tf.keras.layers.Dropout(0.2)(pooled_output)\n",
        "    dense = tf.keras.layers.Dense(128, activation='gelu')(dropout)\n",
        "    output = tf.keras.layers.Dense(2, activation='softmax')(dense)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "    # 2. Advanced Optimizer with Warmup\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "            initial_learning_rate=LEARNING_RATE,\n",
        "            decay_steps=WARMUP_STEPS,\n",
        "            end_learning_rate=0,\n",
        "            power=1.0\n",
        "        ),\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_enhanced_model()\n"
      ],
      "metadata": {
        "id": "TTBE8cAzy433"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Enhanced Training Setup\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
        "\n",
        "tf_train_set = tokenized_train.to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\"],\n",
        "    label_cols=[\"label\"],\n",
        "    shuffle=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "tf_val_set = tokenized_val.to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\"],\n",
        "    label_cols=[\"label\"],\n",
        "    shuffle=False,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=data_collator\n",
        ")\n"
      ],
      "metadata": {
        "id": "aDiCODwhjgAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    tf_train_set,\n",
        "    validation_data=tf_val_set,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "N5qevoJeVByf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== EVALUATION =====\n",
        "# Load your test data (preserving your method)\n",
        "test_texts = pd.read_csv('your_test_data.csv')['text'].tolist()\n",
        "test_labels = pd.read_csv('your_test_data.csv')['label'].tolist()\n",
        "\n",
        "test_dataset = Dataset.from_dict({\"text\": test_texts, \"label\": test_labels})\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "tf_test_set = tokenized_test.to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\"],\n",
        "    label_cols=[\"label\"],\n",
        "    shuffle=False,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "9FVzO9mTVEn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation\n",
        "results = model.evaluate(tf_test_set)\n",
        "print(f\"\\nFinal Test Accuracy: {results[1]:.2%}\")\n",
        "\n",
        "if results[1] < 0.95:\n",
        "    print(\"\\nðŸ”§ Additional Optimization Options:\")\n",
        "    print(\"1. Add StratifiedKFold cross-validation\")\n",
        "    print(\"2. Incorporate back-translation augmentation\")\n",
        "    print(\"3. Try ensemble of multiple models\")\n",
        "else:\n",
        "    print(\"\\nðŸŽ‰ Success! Achieved 95%+ accuracy\")"
      ],
      "metadata": {
        "id": "yYDiW5X8VH5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "\n",
        "# Enhanced TCN Block with Residuals\n",
        "\n",
        "def TCN_block(x, filters, kernel_size, dilation_rate, l2_reg=1e-4):\n",
        "    shortcut = x\n",
        "\n",
        "    # Dilated convolution\n",
        "    x = Conv1D(\n",
        "        filters, kernel_size,\n",
        "        padding='same',\n",
        "        dilation_rate=dilation_rate,\n",
        "        kernel_regularizer=l2(l2_reg)\n",
        "    )(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('swish')(x)  # Swish outperforms ReLU\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # Residual connection\n",
        "    if shortcut.shape[-1] != filters:\n",
        "        shortcut = Conv1D(filters, 1, padding='same')(shortcut)\n",
        "\n",
        "    return Add()([x, shortcut])\n",
        "\n",
        "# Self-Attention Mechanism\n",
        "def attention_block(x):\n",
        "    attention = Dense(1, activation='tanh')(x)\n",
        "    attention = Flatten()(attention)\n",
        "    attention = Activation('softmax')(attention)\n",
        "    attention = RepeatVector(x.shape[-1])(attention)\n",
        "    attention = Permute([2, 1])(attention)\n",
        "    return Multiply()([x, attention])\n",
        "\n",
        "# Learning Rate Schedule\n",
        "def lr_scheduler(epoch, lr):\n",
        "    warmup_epochs = 10\n",
        "    if epoch < warmup_epochs:\n",
        "        return lr * (epoch + 1) / warmup_epochs  # Warmup\n",
        "    return lr * tf.math.exp(-0.1)  # Exponential decay\n",
        "\n",
        "# Model architecture\n",
        "def build_tcn_model(input_length, vocab_size, embedding_dim, embedding_matrix):\n",
        "    inputs = Input(shape=(input_length,))\n",
        "\n",
        "    # Embedding Layer (trainable for fine-tuning)\n",
        "    x = Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        weights=[embedding_matrix],\n",
        "        trainable=True\n",
        "    )(inputs)\n",
        "\n",
        "    # Stacked TCN Blocks\n",
        "    x = TCN_block(x, 128, 3, 1)  # dilation_rate=1\n",
        "    x = TCN_block(x, 128, 3, 2)  # dilation_rate=2\n",
        "    x = TCN_block(x, 128, 3, 4)  # dilation_rate=4\n",
        "    x = TCN_block(x, 128, 3, 8)  # dilation_rate=8 (captures long-range dependencies)\n",
        "\n",
        "    # Attention\n",
        "    x = attention_block(x)\n",
        "    x = Lambda(lambda x: tf.reduce_sum(x, axis=1))(x)  # Weighted sum\n",
        "\n",
        "    # Classifier Head\n",
        "    x = Dense(256, activation='swish', kernel_regularizer=l2(1e-4))(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    tcn_model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Label smoothing + Adam with warmup\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1)\n",
        "    optimizer = Adam(learning_rate=1e-3)\n",
        "\n",
        "    tcn_model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=loss,\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC()]\n",
        "    )\n",
        "\n",
        "    return tcn_model"
      ],
      "metadata": {
        "id": "j_Uk7jPbjj4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize & Train\n",
        "tcn_model = build_tcn_model(\n",
        "    input_length=400,\n",
        "    vocab_size=15000,\n",
        "    embedding_dim=100,\n",
        "    embedding_matrix=embedding_matrix\n",
        ")\n",
        "\n",
        "# Class weights (even if balanced, helps robustness)\n",
        "class_weights = {0: 1.2, 1: 0.8}  # Slight adjustment to favor minority class\n",
        "\n",
        "tcn_history = tcn_model.fit(\n",
        "    X_train, np.array(y_train),\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_val, np.array(y_val)),\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[\n",
        "        LearningRateScheduler(lr_scheduler),\n",
        "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Test-Time Augmentation (TTA)\n",
        "def predict_with_tta(model, X, n_samples=5):\n",
        "    predictions = []\n",
        "    for _ in range(n_samples):\n",
        "        preds = model.predict(X, verbose=0).flatten()  # Enable dropout\n",
        "        predictions.append(preds)\n",
        "    return np.mean(predictions, axis=0)\n",
        "\n",
        "y_pred_tta = (predict_with_tta(tcn_model, X_test) > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "ZO_3G0Jw0sh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=4, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, min_lr=1e-6)\n",
        "checkpoint = ModelCheckpoint(\"best_hybrid_model.h5\", monitor='val_accuracy', save_best_only=True, verbose=1)"
      ],
      "metadata": {
        "id": "tVWEW9Q4jpHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "history = tcn_model.fit(\n",
        "    X_train, np.array(y_train),\n",
        "    validation_data=(X_val, np.array(y_val)),\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop, lr_scheduler, checkpoint],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "8VwruMnvjr2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Predict\n",
        "y_pred_prob = tcn_model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Confusion Matrix (TCN):\")\n",
        "print(cm)\n",
        "print(f\"\\nAccuracy     : {accuracy:.4f}\")\n",
        "print(f\"Precision    : {precision:.4f}\")\n",
        "print(f\"Recall       : {recall:.4f}\")\n",
        "print(f\"F1 Score     : {f1:.4f}\")\n",
        "print(f\"Specificity  : {specificity:.4f}\")"
      ],
      "metadata": {
        "id": "M8J87g2Fx1uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot accuracy and loss vs epoch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy vs Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot Loss\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss vs Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qmDca0TWjyZs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}