{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhsDLLzakGQk16pxvSjDrE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keerthanab2201/Sentiment-Analysis-using-Deep-Learning/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CNN-LSTM Model"
      ],
      "metadata": {
        "id": "9qHR57kAnbvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Collection"
      ],
      "metadata": {
        "id": "-R4GteA2nue0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfbfPXXVpMsa"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load csv file and preview\n",
        "import pandas\n",
        "df= pd.read_csv(\"/content/drive/MyDrive/datasets/Amazon-Product-Reviews-Sentiment-Analysis-in-Python-Dataset.csv\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "C6ur-XpFn6vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save as a JSON file(records format)\n",
        "df.to_json(\"amazon_reviews_data.json\", orient=\"records\", lines=True)\n",
        "print(\"✅ Conversion complete: Saved as reviews_data.json\")"
      ],
      "metadata": {
        "id": "t3hM71jOn-2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Pre-Processing\n",
        "\n",
        "* lowercase\n",
        "* stopword removal\n",
        "* punctuation removal\n",
        "* one word review removal\n",
        "* contraction removal\n",
        "* tokenization\n",
        "* part of speech tagging"
      ],
      "metadata": {
        "id": "qPVR83iyoC8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing dependencies\n",
        "!pip install contractions textblob gensim beautifulsoup4\n",
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "id": "13TM3g9qoGH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "0JyIqYeHohZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import Modules\n",
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import gensim\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from contractions import fix as expand_contractions\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "aKS2O-uUomeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load JSON dataset and inspect columns\n",
        "df = pd.read_json(\"amazon_reviews_data.json\", lines=True)\n",
        "print(df.columns)"
      ],
      "metadata": {
        "id": "OCIKOirfonT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop missing values and filter required columns\n",
        "text_col = \"Review\"\n",
        "label_col = \"Sentiment\"\n",
        "df = df[[text_col, label_col]].dropna() #these are the two columns\n",
        "df.columns = [\"text\", \"rating\"]  # Normalize column names"
      ],
      "metadata": {
        "id": "ZgZMhV2tophx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocessing function\n",
        "def preprocess_pipeline(text):\n",
        "    text = str(text)\n",
        "\n",
        "    # 1. Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Stopword Removal\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "\n",
        "    # 3. Remove Punctuation\n",
        "    tokens = [re.sub(r\"[^\\w\\s]\", \"\", t) for t in tokens]\n",
        "    tokens = [t for t in tokens if t.strip() != \"\"]\n",
        "\n",
        "    # Rejoin tokens for further steps\n",
        "    text = \" \".join(tokens)\n",
        "\n",
        "    # 4. Remove One-Word Reviews\n",
        "    if len(tokens) <= 1:\n",
        "        return None\n",
        "\n",
        "    # 5. Contraction Removal\n",
        "    text = expand_contractions(text)\n",
        "\n",
        "    # 6. Tokenization (again)\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 7. Part-of-Speech (POS) Tagging\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    # 8. Score Generation using TextBlob Sentiment\n",
        "    polarity_score = TextBlob(text).sentiment.polarity  # -1 to 1\n",
        "\n",
        "    return {\n",
        "        \"clean_text\": text,\n",
        "        \"tokens\": tokens,\n",
        "        \"pos_tags\": pos_tags,\n",
        "        \"score\": polarity_score\n",
        "    }"
      ],
      "metadata": {
        "id": "qP8sbho2osjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing function\n",
        "processed = df[\"text\"].apply(preprocess_pipeline)\n",
        "df = df[processed.notnull()].copy()\n",
        "df[\"processed\"] = processed[processed.notnull()].values"
      ],
      "metadata": {
        "id": "HqHyCafmo6Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract cleaned data for tokenization\n",
        "texts = df[\"processed\"].apply(lambda x: x[\"clean_text\"]).tolist()\n",
        "labels = df[\"rating\"].tolist()\n",
        "scores = df[\"processed\"].apply(lambda x: x[\"score\"]).tolist()\n",
        "# Result: Three lists containing the text data, labels, and scores respectively"
      ],
      "metadata": {
        "id": "zGKp9etso84a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Word Embeddings\n",
        "# Keras Tokenizer- converts raw text into numerical sequences (each word= unique integer index) that can be later processed by Keras layers like Embedding\n",
        "MAX_VOCAB = 10000\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=\"\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "# sequence padding- adding placeholder values (often zeros) to shorter sequences in a dataset to make them all the same length\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "padded_sequences = pad_sequences(sequences, maxlen=200, padding='post')"
      ],
      "metadata": {
        "id": "I0vCznndpAhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/glove.6B.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"glove\")"
      ],
      "metadata": {
        "id": "GEd_CWcRpDdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GloVe and Create Embedding Matrix\n",
        "#GloVe(Global Vectors for Word Representation)- converts words into numerical vectors(embeddings) that capture semantic relationships between words- unsupervised learning algorithm\n",
        "\n",
        "embedding_index = {}\n",
        "with open(\"glove/glove.6B.100d.txt\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype=\"float32\")[:50]  # ← Truncate to 50D\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "embedding_dim = 50  # matching model spec\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "NaIpCvHYpHL2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}