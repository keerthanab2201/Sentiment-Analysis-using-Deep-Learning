{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPhj2ngZvLG2AcOxUS1LdR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keerthanab2201/Sentiment-Analysis-using-Deep-Learning/blob/main/tcn_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Collection"
      ],
      "metadata": {
        "id": "-R4GteA2nue0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfbfPXXVpMsa",
        "outputId": "822fb586-b2e1-48a9-d65a-9f23667fbffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load csv file and preview\n",
        "import pandas as pd\n",
        "df= pd.read_csv(\"/content/drive/MyDrive/datasets/Amazon-Product-Reviews-Sentiment-Analysis-in-Python-Dataset.csv\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6ur-XpFn6vr",
        "outputId": "36014d49-b18f-4ced-a04c-e2324a04c498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Review  Sentiment\n",
            "0  Fast shipping but this product is very cheaply...          1\n",
            "1  This case takes so long to ship and it's not e...          1\n",
            "2  Good for not droids. Not good for iPhones. You...          1\n",
            "3  The cable was not compatible between my macboo...          1\n",
            "4  The case is nice but did not have a glow light...          1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save as a JSON file(records format)\n",
        "df.to_json(\"amazon_reviews_data.json\", orient=\"records\", lines=True)\n",
        "print(\"✅ Conversion complete: Saved as reviews_data.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3hM71jOn-2G",
        "outputId": "e97e3d87-da1e-4e5e-8592-30ffe27828b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Conversion complete: Saved as reviews_data.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Pre-Processing\n",
        "\n",
        "* lowercase\n",
        "* stopword removal\n",
        "* punctuation removal\n",
        "* one word review removal\n",
        "* contraction removal\n",
        "* tokenization\n",
        "* part of speech tagging"
      ],
      "metadata": {
        "id": "qPVR83iyoC8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing dependencies\n",
        "!pip install contractions textblob gensim beautifulsoup4\n",
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13TM3g9qoGH4",
        "outputId": "cace7207-e7e9-4926-c3f9-3e1970767b01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JyIqYeHohZ5",
        "outputId": "80935630-24d5-45cd-8a63-99b029effbce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import Modules\n",
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import gensim\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from contractions import fix as expand_contractions\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "aKS2O-uUomeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load JSON dataset and inspect columns\n",
        "df = pd.read_json(\"amazon_reviews_data.json\", lines=True)\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCIKOirfonT5",
        "outputId": "9a6d6eb3-1cdd-43bb-ccb9-6cde7779ff3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Review', 'Sentiment'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop missing values and filter required columns\n",
        "text_col = \"Review\"\n",
        "label_col = \"Sentiment\"\n",
        "df = df[[text_col, label_col]].dropna() #these are the two columns\n",
        "df.columns = [\"text\", \"rating\"]  # Normalize column names"
      ],
      "metadata": {
        "id": "ZgZMhV2tophx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocessing function\n",
        "def preprocess_pipeline(text):\n",
        "    text = str(text)\n",
        "    text = expand_contractions(text.lower())            # Lowercase + contractions\n",
        "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)          # Elongated word normalization\n",
        "    tokens = word_tokenize(text)                        # Tokenization\n",
        "    # Optional: Stopwords (skip for deep models)\n",
        "    # tokens = [t for t in tokens if t not in stop_words]\n",
        "    tokens = [re.sub(r\"[^\\w\\s!?]\", \"\", t) for t in tokens]  # 5. Remove selective punctuation\n",
        "    tokens = [t for t in tokens if t.strip() != \"\"]\n",
        "    if len(tokens) <= 1:\n",
        "        return None\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    clean_text = \" \".join(tokens)\n",
        "    polarity_score = TextBlob(clean_text).sentiment.polarity\n",
        "    return {\n",
        "        \"clean_text\": clean_text,\n",
        "        \"tokens\": tokens,\n",
        "        \"pos_tags\": pos_tags,\n",
        "        \"score\": polarity_score\n",
        "    }\n"
      ],
      "metadata": {
        "id": "qP8sbho2osjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "LApm0TbMjVtg",
        "outputId": "b3385915-4c78-4e29-a886-6b53bb767777",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing function\n",
        "processed = df[\"text\"].apply(preprocess_pipeline)\n",
        "df = df[processed.notnull()].copy()\n",
        "df[\"processed\"] = processed[processed.notnull()].values"
      ],
      "metadata": {
        "id": "HqHyCafmo6Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract cleaned data for tokenization\n",
        "texts = df[\"processed\"].apply(lambda x: x[\"clean_text\"]).tolist()\n",
        "labels = df[\"rating\"].tolist()\n",
        "scores = df[\"processed\"].apply(lambda x: x[\"score\"]).tolist()\n",
        "# Result: Three lists containing the text data, labels, and scores respectively"
      ],
      "metadata": {
        "id": "zGKp9etso84a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Embeddings\n",
        "# Keras Tokenizer- converts raw text into numerical sequences (each word= unique integer index) that can be later processed by Keras layers like Embedding\n",
        "MAX_VOCAB = 10000\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=\"\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "# sequence padding- adding placeholder values (often zeros) to shorter sequences in a dataset to make them all the same length\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "padded_sequences = pad_sequences(sequences, maxlen=200, padding='post')"
      ],
      "metadata": {
        "id": "I0vCznndpAhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/glove.6B.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"glove\")"
      ],
      "metadata": {
        "id": "GEd_CWcRpDdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size= 15000\n",
        "# Load GloVe and Create Embedding Matrix\n",
        "#GloVe(Global Vectors for Word Representation)- converts words into numerical vectors(embeddings) that capture semantic relationships between words- unsupervised learning algorithm\n",
        "\n",
        "embedding_index = {}\n",
        "with open(\"glove/glove.6B.100d.txt\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "embedding_dim = 100  # matching model spec\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < vocab_size:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "NaIpCvHYpHL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove neutral reviews and relabel\n",
        "df = df[df['rating'] != 3]\n",
        "df['label'] = df['rating'].apply(lambda x: 1 if x >= 4 else 0) # creates binary labels: 1 for positive (rating ≥4), 0 for negative"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHxLE1ITBy1I",
        "outputId": "445bda90-a3e8-4a9d-d6ce-a32aa4b85c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-17-3232871527.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['label'] = df['rating'].apply(lambda x: 1 if x >= 4 else 0) # creates binary labels: 1 for positive (rating ≥4), 0 for negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cleaned texts again\n",
        "texts = df[\"processed\"].apply(lambda x: x[\"clean_text\"]).tolist()\n",
        "labels = df[\"label\"].tolist()"
      ],
      "metadata": {
        "id": "7Ey15NRuB5ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare tokenizer and sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "FHWAYnT7CJip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 15000\n",
        "input_length = 400\n",
        "# creates vocabulary of 15,000 most frequent words, converts text to integer sequences and pads sequences to length 400 (truncating longer texts or padding shorter ones)"
      ],
      "metadata": {
        "id": "ZYwivu_QCNkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='')\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "padded_sequences = pad_sequences(sequences, maxlen=input_length, padding='post')\n",
        "\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "fkxmxiElCQp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup embedding matrix\n",
        "# initializes embedding matrix with zeros and fill it with pre-trained GloVe vectors where available\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "sA05B9ZGCt7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split- 70% training / 10% validation / 20% testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Split 20% test set\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    padded_sequences, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# Step 2: Split remaining 80% into 70% train and 10% validation\n",
        "# 70% of full data = 87.5% of remaining 80%\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.125, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"Train size     : {len(X_train)}\")\n",
        "print(f\"Validation size: {len(X_val)}\")\n",
        "print(f\"Test size      : {len(X_test)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmZ4DHZhCv-N",
        "outputId": "310c8a5c-47f3-442c-d7aa-0ec68b041d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size     : 13867\n",
            "Validation size: 1981\n",
            "Test size      : 3963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TCN Model"
      ],
      "metadata": {
        "id": "qxZ7kWzKi-jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tcn\n",
        "!pip install -q tensorflow-addons"
      ],
      "metadata": {
        "id": "_5sfBpoagvGm",
        "outputId": "4b58abcc-668c-4fc5-b472-6d184e9d3a94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tcn in /usr/local/lib/python3.11/dist-packages (3.5.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras-tcn) (1.26.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from keras-tcn) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-tcn) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->keras-tcn) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->keras-tcn) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->keras-tcn) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->keras-tcn) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-tcn) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-tcn) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-tcn) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-tcn) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->keras-tcn) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->keras-tcn) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->keras-tcn) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow->keras-tcn) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->keras-tcn) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->keras-tcn) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->keras-tcn) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, BatchNormalization, Bidirectional, LSTM, GlobalMaxPooling1D\n",
        "from tcn import TCN\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "u_5k5eioi5X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Layer\n",
        "import tensorflow as tf\n",
        "\n",
        "class Attention(Layer):\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name=\"attention_weight\",\n",
        "                                 shape=(input_shape[-1], 1),\n",
        "                                 initializer=\"random_normal\",\n",
        "                                 trainable=True)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Compute attention scores\n",
        "        score = tf.nn.tanh(tf.matmul(inputs, self.W))  # shape: (batch, time_steps, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)  # shape: (batch, time_steps, 1)\n",
        "        context_vector = attention_weights * inputs  # shape: (batch, time_steps, features)\n",
        "        return tf.reduce_sum(context_vector, axis=1)  # shape: (batch, features)"
      ],
      "metadata": {
        "id": "TTBE8cAzy433"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "embedding_dim = 100\n",
        "input_length = 400\n",
        "num_filters = 128\n",
        "kernel_size = 5\n",
        "lstm_units = 64\n",
        "dropout_rate = 0.5\n",
        "l2_reg = 0.001"
      ],
      "metadata": {
        "id": "aDiCODwhjgAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "inputs = Input(shape=(input_length,))\n",
        "x = Embedding(input_dim=vocab_size,\n",
        "              output_dim=embedding_dim,\n",
        "              weights=[embedding_matrix],\n",
        "              input_length=input_length,\n",
        "              trainable=True)\n",
        "\n",
        "x = Conv1D(filters=num_filters, kernel_size=kernel_size, activation='relu')(x)\n",
        "x = MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "x = Bidirectional(LSTM(units=lstm_units, return_sequences=True, dropout=0.3))(x)\n",
        "x = Attention()(x)\n",
        "\n",
        "x = Dense(128, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
        "x = Dropout(dropout_rate)(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "tcn_model = Model(inputs=inputs, outputs=outputs)\n",
        "tcn_model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "tcn_model.summary()"
      ],
      "metadata": {
        "id": "j_Uk7jPbjj4V",
        "outputId": "977e347a-5c15-433e-b8c1-af64d6971a4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Layer embedding weight shape (15000, 100) is not compatible with provided weight shape (28238, 100).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-28-3818162777.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Model definition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m x = Embedding(input_dim=vocab_size,\n\u001b[0m\u001b[1;32m      4\u001b[0m               \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, weights, lora_rank, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    707\u001b[0m                     \u001b[0;34mf\"Layer {self.name} weight shape {variable.shape} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m                     \u001b[0;34m\"is not compatible with provided weight \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Layer embedding weight shape (15000, 100) is not compatible with provided weight shape (28238, 100)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=4, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, min_lr=1e-6)\n",
        "checkpoint = ModelCheckpoint(\"best_hybrid_model.h5\", monitor='val_accuracy', save_best_only=True, verbose=1)"
      ],
      "metadata": {
        "id": "tVWEW9Q4jpHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "history = tcn_model.fit(\n",
        "    X_train, np.array(y_train),\n",
        "    validation_data=(X_val, np.array(y_val)),\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop, lr_scheduler, checkpoint],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "8VwruMnvjr2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_tcn.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_tcn.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title(\"Accuracy vs Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_tcn.history['loss'], label='Train Loss')\n",
        "plt.plot(history_tcn.history['val_loss'], label='Val Loss')\n",
        "plt.title(\"Loss vs Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qmDca0TWjyZs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}